{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvBVTARjv2W3h+kkoddQco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave21-py/Siri-project/blob/main/dlproject2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qViC-JVf-z7J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'siri_dataset_large.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "afz5VELJ_JVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset loaded succesfully\")\n",
        "print(\"Total no. of examples\", len(df))"
      ],
      "metadata": {
        "id": "yfYXupgJ_pzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset loaded successfully! Here are the first 5 rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "x-KxpN4a_q5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training"
      ],
      "metadata": {
        "id": "oqGzrydsgkcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "uECoWkI7goyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "3IQEImwdXgU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any rows that have empty values\n",
        "df = df.dropna()\n",
        "\n",
        "print(\"Bad rows removed.\")\n",
        "print(\"New total examples:\", len(df))"
      ],
      "metadata": {
        "id": "QiVW4DrWIcm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['utterance']\n",
        "y = df['intent']"
      ],
      "metadata": {
        "id": "ODRYyCgqYUlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "WaTRb96naoLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "t5564lrPaxES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data preparation complete!\")\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "jp-0KR99bTAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y[0])\n",
        "print(y_encoded[0])"
      ],
      "metadata": {
        "id": "YVI5FnMpbwLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorisation"
      ],
      "metadata": {
        "id": "fU2SYM2OcVHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "PR44aGLkczDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline, step tfidf and logistic regression"
      ],
      "metadata": {
        "id": "UEkDkZGxdUnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(max_iter=1000)),\n",
        "])"
      ],
      "metadata": {
        "id": "lYaFEq0wdf0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the base model\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Training complete\")"
      ],
      "metadata": {
        "id": "JWIWkECVawVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating our model"
      ],
      "metadata": {
        "id": "fJI__hE-bCMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "5TyQ00ETbSy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Making predictions on the test data.....\")\n",
        "predictions = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "pY967wtNbn-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, predictions)"
      ],
      "metadata": {
        "id": "EERaDOLHdUtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"New Baseline model accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "P8t9w_4mdvCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a detailed classification report\n",
        "print(\"Detailed classification report......\")\n",
        "report = classification_report(y_test, predictions, target_names = label_encoder.classes_)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "Abmu82X3d2Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'What is the temperate outside right now?'\n",
        "def predict_intent(sentence):\n",
        "  prediction = pipeline.predict([sentence])\n",
        "  predicted_index = prediction[0]\n",
        "  predicted_intent = label_encoder.inverse_transform([predicted_index]) # turns back to text\n",
        "  return predicted_intent[0]"
      ],
      "metadata": {
        "id": "3Xax-Vf9eazP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = 'Set me a time for 30 seconds'\n",
        "print(f\"Sentence: '{test_sentence}'\")\n",
        "print(f\"Predicted Intent: {predict_intent(test_sentence)}\")"
      ],
      "metadata": {
        "id": "zpRSPr3tfNTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sLB2BbSYfWdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Settings\n",
        "vocab_size = 1000\n",
        "max_length = 20\n",
        "oov_token = '<00V>'"
      ],
      "metadata": {
        "id": "H7K4A2z_hNv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the tokenizer"
      ],
      "metadata": {
        "id": "9gQDC6H8hsDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "3dP1vtT5kE6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conver the training text and testing text to numerical sequence"
      ],
      "metadata": {
        "id": "t2qqB9WBkPSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "wNfr_kaykZVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padd the new sequences"
      ],
      "metadata": {
        "id": "FPTdlYwLk6Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_padded = pad_sequences(X_train_sequences, maxlen = max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen = max_length, padding='post')"
      ],
      "metadata": {
        "id": "fp0TnUEIk95d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lets check our work\n",
        "\n",
        "\n",
        "print(\"Original sentence:\", X_train.iloc[0])\n",
        "#Now its new padded numerical sequence\n",
        "print(\"Padded sequence:\", X_train_padded[0])\n",
        "print(\"Data is now ready for deep learning model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFbm7-whmDnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM NN"
      ],
      "metadata": {
        "id": "Mv3c3p7YmXSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "Q9CJK8uanCh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "1xHg6vIuntMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "_uoP9WnDoEk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim = vocab_size, output_dim = 16, input_shape = (max_length,)),\n",
        "    LSTM(32),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "aVr1Bp8Co32x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model"
      ],
      "metadata": {
        "id": "ONL3EMMUplZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nYESZ8rJppWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training process"
      ],
      "metadata": {
        "id": "7FCxzIEiqXPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "IdA2Kyglqw27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training of the LSTM Model.....\")\n",
        "history = model.fit(\n",
        "    X_train_padded,\n",
        "    y_train,\n",
        "    epochs = num_epochs,\n",
        "    validation_data = (X_test_padded, y_test)\n",
        ")\n",
        "print(\"Training complete\")"
      ],
      "metadata": {
        "id": "ZS7mN6cKrBIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation our output"
      ],
      "metadata": {
        "id": "T2uCoCF5rVKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gUyhdQuFssgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy values from the training history"
      ],
      "metadata": {
        "id": "F2dkCotbs-TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']"
      ],
      "metadata": {
        "id": "_d_HTeyrtInI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the loss values from the traininig history"
      ],
      "metadata": {
        "id": "zldX89gXtR6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "metadata": {
        "id": "0-S3JMHttYYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_range = range(len(acc))"
      ],
      "metadata": {
        "id": "X0WHwVRGtkkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,6))"
      ],
      "metadata": {
        "id": "ZCi4LPX6tpN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuray')\n",
        "plt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy (New Dataset)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label = 'Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss (New Dataset)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y3ebl4hAtu4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows its not performinig even with our LSTM model"
      ],
      "metadata": {
        "id": "NpbXZ2Qdu7xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "F0favk52wDLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "AC2uWn1vwQSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "RW72aUVrxI3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization FOR THE DATA FOR DISTILBERT"
      ],
      "metadata": {
        "id": "amln57ZmxkxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = transformer_tokenizer(list(X_train), truncation = True,\n",
        "padding = 'max_length', max_length = 50, return_tensors = 'tf') # if any sentence is longer than 50, it will be c ut off as there is truncation"
      ],
      "metadata": {
        "id": "NIzt7Py41RdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = transformer_tokenizer(list(X_test),truncation = True,\n",
        "                                       padding = 'max_length', max_length = 50, return_tensors = 'tf')"
      ],
      "metadata": {
        "id": "FKcItOEV18eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model"
      ],
      "metadata": {
        "id": "y_RJ4NVX2R5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "REp36Jp73APO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dK2JpJFT7GxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers datasets"
      ],
      "metadata": {
        "id": "qHCXrD-F7U5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the main PyTorch library\n",
        "import torch\n",
        "\n",
        "# From the 'transformers' library, we import the PyTorch versions\n",
        "# of the tokenizer and the sequence classification model.\n",
        "# Notice there is no \"TF\" prefix this time.\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "Y-y0ByRN-W3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a tokenizer object from the pre-trained 'distilbert-base-uncased' model.\n",
        "# This works for both TensorFlow and PyTorch.\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "OfYNtaqw_ZvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(X_train), truncation = True, padding = True)\n",
        "test_encodings = tokenizer(list(X_test), truncation = True, padding = True)\n"
      ],
      "metadata": {
        "id": "zWcyT1TP_2Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Pytorch Dataset classs"
      ],
      "metadata": {
        "id": "t1rVDeE4Ajos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntentDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "train_dataset = IntentDataset(train_encodings, y_train)\n",
        "test_dataset = IntentDataset(test_encodings, y_test)"
      ],
      "metadata": {
        "id": "Q0HLAfWlAmv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained pytorch model"
      ],
      "metadata": {
        "id": "_kEPZaOwBY8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are creating our DistilBERT model object using the PyTorch class.\n",
        "# .from_pretrained() downloads the pre-trained model weights.\n",
        "# num_labels tells it to create a new final layer suitable for our 10-class problem.\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))"
      ],
      "metadata": {
        "id": "UtheG-5ZCp0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Trainer and the training arguments class\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define the arguments for the training process.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Directory to save results\n",
        "    num_train_epochs=3,              # We'll train for 3 full cycles\n",
        "    per_device_train_batch_size=16,  # Process 16 examples at a time during training\n",
        "    per_device_eval_batch_size=64,   # Process 64 examples at a time during evaluation\n",
        "    warmup_steps=50,                 # Number of steps to warm up the learning rate\n",
        "    weight_decay=0.01,               # A regularization technique to prevent overfitting\n",
        "    logging_dir='./logs',            # Directory to save logs\n",
        "    report_to='none',\n",
        ")\n",
        "\n",
        "# Create the Trainer object, which bundles everything together.\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The model we just loaded\n",
        "    args=training_args,                  # The training settings we just defined\n",
        "    train_dataset=train_dataset,         # Our PyTorch training dataset\n",
        "    eval_dataset=test_dataset            # Our PyTorch testing dataset\n",
        ")\n",
        "print(\"Trainer re-created with logging disabled.\")"
      ],
      "metadata": {
        "id": "pGPp8vGcCquT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "We4zNiMxDtOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-running with Accuracy"
      ],
      "metadata": {
        "id": "PPbQKwFDGA1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3jt3rSETGXIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "Wg5ZEfNnGYQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the accuracy metric from the evaluate library\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define the function that the Trainer will use to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Re-create the Trainer, but this time we add our new compute_metrics function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics, # <-- This is the new instruction\n",
        ")\n",
        "print(\"Trainer has been re-created with the ability to calculate accuracy!\")"
      ],
      "metadata": {
        "id": "XYT1oNG-GhHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the evaluation again\n",
        "final_evaluation = trainer.evaluate()\n",
        "\n",
        "# Print the final results\n",
        "print(\"--- Final Transformer Model Evaluation ---\")\n",
        "for key, value in final_evaluation.items():\n",
        "    print(f\"{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "aAk813o5Gl8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "# 1. Save the big \"Teacher\" model you just trained\n",
        "model_dir = \"./siri_bert_model\"\n",
        "trainer.save_model(model_dir)\n",
        "\n",
        "# 2. Load it back (specifically for CPU/Mobile optimization)\n",
        "model_fp32 = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
        "\n",
        "# 3. Quantize it! (Compress the math from 32-bit to 8-bit)\n",
        "# This is where the magic happens\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model_fp32, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# 4. Save the new \"Tiny\" model\n",
        "quantized_output_path = \"siri_bert_quantized.pt\"\n",
        "torch.save(quantized_model, quantized_output_path)\n",
        "\n",
        "# 5. Compare the file sizes\n",
        "def get_size(path):\n",
        "    size = os.path.getsize(path)\n",
        "    return size / (1024 * 1024) # Convert to Megabytes (MB)\n",
        "\n",
        "original_size = get_size(os.path.join(model_dir, \"model.safetensors\"))\n",
        "quantized_size = get_size(quantized_output_path)\n",
        "\n",
        "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
        "print(f\"Quantized Model Size: {quantized_size:.2f} MB\")\n",
        "print(f\"Compression Ratio: {original_size / quantized_size:.2f}x smaller!\")"
      ],
      "metadata": {
        "id": "s187cctbGoXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Save the Label Encoder (The Translator)\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# 2. Download the Model and the Translator to your computer\n",
        "print(\"Downloading files... check your browser's download bar!\")\n",
        "files.download(\"siri_bert_quantized.pt\")\n",
        "files.download(\"label_encoder.pkl\")"
      ],
      "metadata": {
        "id": "GB9EPL1DVnX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlHG577zONnW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}